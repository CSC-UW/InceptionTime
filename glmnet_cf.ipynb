{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glmnet_python\n",
    "from glmnet import glmnet\n",
    "\n",
    "# Import relevant modules and setup for calling glmnet\n",
    "%reset -f\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf # data is in TFRecord format\n",
    "\n",
    "import scipy, importlib, pprint, matplotlib.pyplot as plt, warnings\n",
    "import numpy as np\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot\n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "\n",
    "# glmnet has many deprecation warnings from using scipy.* instad of numpy.*\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "names = ['DA', '5HT', 'pH', 'NE']\n",
    "ncores = 28 # 56\n",
    "prefix = os.path.join('/mnt/nfs/proj/in-vitro/Mark/four_analyte/slow/allin')\n",
    "good_probes = ['CF025', 'CF027']\n",
    "# good_probes = ['CF025', 'CF027', 'CF057', 'CF064', 'CF066', 'CF078', 'CF081', 'CF082']\n",
    "val_ratio = .1\n",
    "# hold_probe = 0\n",
    "hold_probe = -1 # split data randomly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110970, 999) (110970, 4)\n",
      "(12330, 999) (12330, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    def atoi(text):\n",
    "        return int(text) if text.isdigit() else text\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "def preprocess(serialized_example):\n",
    "    features = tf.io.parse_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'gram': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label': tf.io.FixedLenFeature([], tf.string)\n",
    "        })\n",
    "    data = tf.io.decode_raw(features['gram'], tf.float32)\n",
    "    label = tf.io.decode_raw(features['label'], tf.float32)\n",
    "    data.set_shape((None, 999))\n",
    "    label.set_shape((None, 4))\n",
    "    return data, label\n",
    "\n",
    "def merge_datasets(vfiles, projy=lambda x: x, asnumpy=False):\n",
    "    yv = []\n",
    "    yl = []\n",
    "    for filename in vfiles:\n",
    "        ds = tf.data.TFRecordDataset(filename)\n",
    "        ds = ds.batch(batch_size=2**13)\n",
    "        ds = ds.map(map_func=preprocess)\n",
    "        for v,l in ds:\n",
    "            v = np.array(v).astype(np.float64)\n",
    "            l = np.array(l).astype(np.float64)\n",
    "            yv.append(v)\n",
    "            l = np.apply_along_axis(projy, axis=1, arr=l) \n",
    "            yl.append(l)\n",
    "        \n",
    "    x = np.vstack(yv)\n",
    "    y = np.vstack(yl)\n",
    "\n",
    "    if asnumpy:\n",
    "        return x,y\n",
    "    else:\n",
    "        d = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        return d\n",
    "\n",
    "if hold_probe < 0:\n",
    "    all_files = sum([\n",
    "        sorted(tf.io.gfile.glob(os.path.join(prefix, probe, 'total_records', '*')),\n",
    "               key=natural_keys) for probe in good_probes\n",
    "    ], [])\n",
    "\n",
    "    x, y = merge_datasets(all_files, asnumpy=True)\n",
    "    \n",
    "    idxs = np.random.permutation(x.shape[0])\n",
    "    lim = int(x.shape[0]*(1-val_ratio))\n",
    "    d1idx = idxs[idxs[:lim]]\n",
    "    d2idx = idxs[idxs[lim:]]\n",
    "    x_train, y_train, x_val, y_val = x[d1idx,:], y[d1idx,:], x[d2idx,:], y[d2idx,:]\n",
    "else:\n",
    "    hold_probe = good_probes.pop(hold_probe)\n",
    "    train_files = sum([\n",
    "        sorted(tf.io.gfile.glob(os.path.join(prefix, probe, 'total_records', '*')),\n",
    "               key=natural_keys) for probe in good_probes\n",
    "    ], [])\n",
    "    x_train, y_train = merge_datasets(train_files, asnumpy=True)\n",
    "    \n",
    "    val_files = sum([\n",
    "        sorted(tf.io.gfile.glob(os.path.join(prefix, probe, 'total_records', '*')),\n",
    "               key=natural_keys) for probe in [hold_probe]\n",
    "    ], [])\n",
    "    x_val, y_val = merge_datasets(val_files, asnumpy=True)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[status]\tParallel glmnet cv with 28 cores\n"
     ]
    }
   ],
   "source": [
    "# fit GLMNET in parallel (ncores) with cross validation to find lambda\n",
    "fit = cvglmnet(x = x_train.copy(), y = y_train.copy(), family='mgaussian', parallel=ncores, ptype = 'mse', nfolds = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10870285])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit['lambda_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = cvglmnetPredict(fit, newx = x_val, s='lambda_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DA: 340.70265\n",
      "5HT: 134.71918\n",
      "pH: 0.02452\n",
      "NE: 399.73640\n"
     ]
    }
   ],
   "source": [
    "for (error, name) in zip(np.mean(np.sqrt((y_hat[:,:,0]-y_val)**2),axis=0),names):\n",
    "    print('%s: %4.5f'%(name,error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
