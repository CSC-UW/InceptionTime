{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glmnet_python\n",
    "from glmnet import glmnet\n",
    "\n",
    "# Import relevant modules and setup for calling glmnet\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from itertools import compress\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf # data is in TFRecord format\n",
    "\n",
    "import scipy, importlib, pprint, matplotlib.pyplot as plt, warnings\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import numpy as np\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot\n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "\n",
    "# glmnet has many deprecation warnings from using scipy.* instad of numpy.*\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def plot_distributions(y, y_hat):\n",
    "\n",
    "    fig, axs = plt.subplots(2, 4, sharey=False, sharex=False, tight_layout=True)\n",
    "    fig.set_size_inches(20,10)\n",
    "\n",
    "    for ic in range(0,8):\n",
    "        idx = np.unravel_index(ic, axs.shape)\n",
    "        if ic < 4:\n",
    "            axs[idx].set_title(\"Real %s\"%names[ic])\n",
    "            x = y[:,ic]        \n",
    "        else:\n",
    "            axs[idx].set_title(\"Predicted %s\"%names[ic-4])\n",
    "            x = y_hat[:,ic-4]\n",
    "        axs[idx].hist(x, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "ncores = 28 # 56\n",
    "names = ['DA', '5HT', 'pH', 'NE']\n",
    "results_prefix = '/mnt/nfs/proj/in-vitro/Leonardo/glmnet/fits/'\n",
    "data_prefix = '/mnt/nfs/proj/in-vitro/Mark/four_analyte/slow/allin'\n",
    "# good_probes = ['CF025']\n",
    "# good_probes = ['CF025', 'CF027']\n",
    "good_probes = ['CF025', 'CF027', 'CF057', 'CF064', 'CF066', 'CF078', 'CF081', 'CF082']\n",
    "# nrecords_per_session = 1 # Loads N sweeps per concentration per probe\n",
    "nrecords_per_session = -1 # Loads all records\n",
    "val_ratio = .1 # save some random percentage of probes different from holdout for validation \n",
    "# val_ratio = .0 # validation set will be empty\n",
    "hold_probe = 0\n",
    "# hold_probe = -1 # turn the validation set in holdout set (validation set will be empty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(388395, 999) (388395, 4)\n",
      "(61650, 999) (61650, 4)\n",
      "(43155, 999) (43155, 4)\n",
      "--- 10.801665306091309 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    def atoi(text):\n",
    "        return int(text) if text.isdigit() else text\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "def preprocess(serialized_example):\n",
    "    features = tf.io.parse_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'gram': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label': tf.io.FixedLenFeature([], tf.string)\n",
    "        })\n",
    "    data = tf.io.decode_raw(features['gram'], tf.float32)\n",
    "    label = tf.io.decode_raw(features['label'], tf.float32)\n",
    "    data.set_shape((None, 999))\n",
    "    label.set_shape((None, 4))\n",
    "    return data, label\n",
    "\n",
    "def merge_datasets(vfiles, projy=lambda x: x, asnumpy=False):\n",
    "    yv = []\n",
    "    yl = []\n",
    "    for filename in vfiles:\n",
    "        ds = tf.data.TFRecordDataset(filename)\n",
    "        ds = ds.batch(batch_size=2**13)\n",
    "        ds = ds.map(map_func=preprocess)\n",
    "        for v,l in ds:\n",
    "            v = np.array(v).astype(np.float64)\n",
    "            l = np.array(l).astype(np.float64)\n",
    "            l = np.apply_along_axis(projy, axis=1, arr=l) \n",
    "\n",
    "            # get n unique records per label (tuple), or all data (-1)\n",
    "            if n_records_per_label_per_probe > -1:\n",
    "                # just to make sure there is only once concertraiton tuple in this record\n",
    "                _, ulidx = np.unique(l, return_index=True, axis=0)\n",
    "                u_v = []\n",
    "                u_l = []\n",
    "                for idx in ulidx:\n",
    "                    # TODO: I think this can come from unique\n",
    "                    one_label_idxs = np.where((l == l[idx,:]).all(axis=1))[0]\n",
    "                    u_v.append(v[one_label_idxs[:n_records_per_label_per_probe], :])\n",
    "                    u_l.append(l[one_label_idxs[:n_records_per_label_per_probe], :])\n",
    "                v = np.concatenate(u_v)\n",
    "                l = np.concatenate(u_l)\n",
    "\n",
    "            yv.append(v)\n",
    "            yl.append(l)\n",
    "\n",
    "    x = np.vstack(yv)\n",
    "    y = np.vstack(yl)\n",
    "\n",
    "    if asnumpy:\n",
    "        return x,y\n",
    "    else:\n",
    "        d = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        return d\n",
    "\n",
    "x_val, y_val = np.array([]), np.array([])\n",
    "if hold_probe < 0:\n",
    "    all_files = sum([\n",
    "        sorted(tf.io.gfile.glob(os.path.join(data_prefix, probe, 'total_records', '*')),\n",
    "               key=natural_keys) for probe in good_probes\n",
    "    ], [])\n",
    "\n",
    "    x, y = merge_datasets(all_files, asnumpy=True)\n",
    "    \n",
    "    idxs = np.random.permutation(x.shape[0])\n",
    "    lim = int(x.shape[0]*(1-val_ratio))\n",
    "    d1idx = idxs[idxs[:lim]]\n",
    "    d2idx = idxs[idxs[lim:]]\n",
    "    x_train, y_train, x_test, y_test = x[d1idx,:], y[d1idx,:], x[d2idx,:], y[d2idx,:]\n",
    "else:\n",
    "    hold_probe_str = good_probes.pop(hold_probe)\n",
    "    train_val_files = sum([\n",
    "        sorted(tf.io.gfile.glob(os.path.join(data_prefix, probe, 'total_records', '*')),\n",
    "               key=natural_keys) for probe in good_probes\n",
    "    ], [])\n",
    "    x, y = merge_datasets(train_val_files, asnumpy=True)\n",
    "\n",
    "    if val_ratio > 0.:\n",
    "        idxs = np.random.permutation(x.shape[0])\n",
    "        lim = int(x.shape[0]*(1-val_ratio))\n",
    "        d1idx = idxs[idxs[:lim]]\n",
    "        d2idx = idxs[idxs[lim:]]\n",
    "        x_train, y_train, x_val, y_val = x[d1idx,:], y[d1idx,:], x[d2idx,:], y[d2idx,:]\n",
    "    else:\n",
    "        x_train, y_train = x, y\n",
    "\n",
    "    hold_files = sum([\n",
    "        sorted(tf.io.gfile.glob(os.path.join(data_prefix, probe, 'total_records', '*')),\n",
    "               key=natural_keys) for probe in [hold_probe_str]\n",
    "    ], [])\n",
    "    x_test, y_test = merge_datasets(hold_files, asnumpy=True)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "if x_val.size > 0:\n",
    "    print(x_val.shape, y_val.shape)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting training data into cells per concentration...  done.\n"
     ]
    }
   ],
   "source": [
    "print('Spliting training data into cells per concentration... ', end='')\n",
    "\n",
    "# nint = 5\n",
    "nint = 2\n",
    "analytes = [0,1,3]\n",
    "concentrations = [[]]*len(analytes)\n",
    "ranges = [[]]*len(analytes)\n",
    "for ic in range(len(analytes)):\n",
    "    concentrations[ic] = np.linspace(min(y_train[:,analytes[ic]]), max(y_train[:,analytes[ic]]), nint+1)\n",
    "#     print(c[ic])\n",
    "    ranges[ic] = [concentrations[ic][ir] + np.mean(np.diff(concentrations[ic]))/2 for ir in range(len(concentrations[ic])-1)]\n",
    "#     print(r[ic])\n",
    "\n",
    "# use itertools product instead of nested for loops to make it easier to change the number of analytes\n",
    "from itertools import product\n",
    "split_data = {x: np.array([]) for x in product(range(nint), repeat=len(analytes))}\n",
    "error = []\n",
    "# nsamples = []\n",
    "# print(len(split_data))\n",
    "for x in split_data:\n",
    "\n",
    "    # build binary mask for this cell\n",
    "    good_data = np.full((y_train.shape[0],), True)\n",
    "#     print(good_data.shape, good_data[0].__class__)\n",
    "    for (ix,nx) in enumerate(x):\n",
    "#         print(sum(good_data))\n",
    "#         print(a[ix], nx)\n",
    "        zeros = y_train[:,analytes[ix]] == 0.0\n",
    "        above = y_train[:,analytes[ix]] > concentrations[ix][nx]\n",
    "#         print(above.shape, above[0].__class__, c[ix][nx])\n",
    "        below = y_train[:,analytes[ix]] < concentrations[ix][nx+1]\n",
    "#         print(below.shape, below[0].__class__, c[ix][nx+1])\n",
    "        interval = np.bitwise_or(np.bitwise_and(above, below), zeros)\n",
    "#         print(interval.shape, interval[0].__class__, sum(interval))\n",
    "        good_data = np.bitwise_and(good_data, interval)\n",
    "\n",
    "#     print(sum(good_data))\n",
    "    # save data\n",
    "    split_data[x] = (x_train[good_data,:], y_train[good_data,:])\n",
    "    \n",
    "    y = split_data[x][1]\n",
    "    means = np.array([np.mean(y[y[:,x]>0, x]) for x in range(4)])\n",
    "#     print(means)\n",
    "    expected_means = np.array([ranges[i][x[i]] for i in range(len(analytes))])\n",
    "#     print(expected_means)\n",
    "    error.append(np.sqrt( (means[np.array(analytes)] - expected_means)**2 ))\n",
    "#     nsamples.append(y.shape[0])\n",
    "\n",
    "print(' done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0) 191996\n",
      "(0, 0, 1) 199296\n",
      "(0, 1, 0) 200520\n",
      "(0, 1, 1) 185486\n",
      "(1, 0, 0) 199817\n",
      "(1, 0, 1) 184415\n",
      "(1, 1, 0) 185442\n",
      "(1, 1, 1) 170391\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inspect split data\n",
    "nsamples = np.zeros(len(split_data))\n",
    "for (ix, x) in enumerate(split_data):\n",
    "    print(x, split_data[x][1].shape[0])\n",
    "    \n",
    "e = np.array(error)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20,5))\n",
    "for idx in range(3):\n",
    "    axs[idx].hist(e[:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 10 cross-validations for model 0 of 8 ((0, 0, 0)) ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[status]\tParallel glmnet cv with 28 cores\n"
     ]
    }
   ],
   "source": [
    "# fit GLMNET in parallel (ncores) with cross validation to find lambda\n",
    "\n",
    "# alphas = [.9, 1.]\n",
    "alpha = 1.0\n",
    "models = {x: [] for x in split_data}\n",
    "ncross = 10\n",
    "nx = len(split_data)\n",
    "for (ix, x) in enumerate(split_data):\n",
    "    this_x_train, this_y_train = split_data[x]\n",
    "    print(f'Computing {ncross} cross-validations for model {ix} of {nx} ({x}) ... ', end='')\n",
    "    start_time = time.time()\n",
    "    models[x] = cvglmnet(x = this_x_train.copy(), y = this_y_train.copy(), family='mgaussian', parallel=ncores, ptype = 'mse', nfolds = ncross, alpha=alpha)\n",
    "    print(\" took %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join(results_prefix, f'cf_alpha_1.0_{nint}x{nint}x{nint}.pickle')\n",
    "print(output_file)\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# _, uidx = np.unique(y_train, return_index=True, axis=0)\n",
    "# for (idx, x) in enumerate(y_train[uidx, :]):\n",
    "#     if x[0] < 2000 and x[3] < 2000 and x[1] > 3000:\n",
    "#         print(idx, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the model with predictions closest to original range (sum over all analytes) and compute RMSE of each analyte for that model \n",
    "\n",
    "x_to_pred = x_test\n",
    "y_to_pred = y_test\n",
    "\n",
    "# # DEBUG\n",
    "# x_to_pred = x_train\n",
    "# y_to_pred = y_train\n",
    "\n",
    "# x_to_pred = x_train[uidx[307], :][None, :]\n",
    "# y_to_pred = y_train[uidx[307], :][None, :]\n",
    "# x_to_pred = x_train[[uidx[307], uidx[210]], :]\n",
    "# y_to_pred = y_train[[uidx[307], uidx[210]], :]\n",
    "\n",
    "# generate the predictions for each model\n",
    "y_hats = np.zeros((len(models), y_to_pred.shape[0], y_to_pred.shape[1]))\n",
    "for (ix, x) in enumerate(models):\n",
    "    print(f'{ix} of {nx} ({x})... ', end='')\n",
    "    model = models[x]\n",
    "    start_time = time.time()\n",
    "#     y_hat = cvglmnetPredict(fit, newx = x_to_pred, s='lambda_min')\n",
    "    y_hat = cvglmnetPredict(model, newx = x_to_pred, s='lambda_1se')\n",
    "    \n",
    "    # DEBUG\n",
    "#     y_hat = y_test[:,:,None]*(1.+0.1*np.random.randn(y_test.shape[0], y_test.shape[1], 1))\n",
    "\n",
    "    y_hats[ix,:,:] = y_hat[:,:,0]\n",
    "    print(\" took %s seconds\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ranges)\n",
    "# compute the differences to original intervals\n",
    "diff_y_hats = np.zeros(y_hats[:,:,analytes].shape)\n",
    "diff_y_true = np.zeros(y_hats[:,:,analytes].shape)\n",
    "for (ix,x) in enumerate(models):\n",
    "#     print(x)\n",
    "    for ia in range(len(analytes)):\n",
    "#         print(ix, ia, analytes[ia], x[ia], ranges[ia][x[ia]])\n",
    "        diff_y_hats[ix,:,ia] = (y_hats[ix,:,analytes[ia]] - ranges[ia][x[ia]])**2\n",
    "        # save that for comparing how often the predicted model matches the \"ideal\" model \n",
    "        diff_y_true[ix,:,ia] = (y_to_pred[:,analytes[ia]] - ranges[ia][x[ia]])**2 \n",
    "\n",
    "# find the model with sum of predictions closest to original intervals\n",
    "model_e = np.sqrt(np.sum(diff_y_hats,axis=2)) # take sqrt to make it easier to debug, doesn't change the ordering\n",
    "min_idx = model_e.argmin(axis=0)\n",
    "\n",
    "# compute the RMSE for each sample (there is certainly a more pythonic way to do that, but that works)\n",
    "rmse = np.zeros((4,), dtype=np.float64)\n",
    "for (sample_idx, model_idx) in enumerate(min_idx):\n",
    "    rmse += (y_hats[model_idx, sample_idx, :] - y_to_pred[sample_idx, :])**2/y_to_pred.shape[0]\n",
    "\n",
    "rmse = np.sqrt(rmse) \n",
    "\n",
    "for (armse, name) in zip(rmse,names):\n",
    "    print('%s: %4.5f'%(name,armse), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the model with sum of predictions closest to original intervals\n",
    "real_model_e = np.sqrt(np.sum(diff_y_true,axis=2)) # take sqrt to make it easier to debug, doesn't change the ordering\n",
    "real_min_idx = real_model_e.argmin(axis=0)\n",
    "ratio_same = sum(real_min_idx == min_idx)/min_idx.shape[0]\n",
    "print(f'How often the predicted model matches the expected model? {ratio_same}')\n",
    "# fig, axs = plt.subplots(1, 2, sharey=False, sharex=False, tight_layout=True, figsize=(20,10))\n",
    "x = np.array(list(split_data.keys()))\n",
    "d = np.zeros(min_idx.shape)\n",
    "for sample in range(len(d)):\n",
    "    d[sample] = sum(abs(x[min_idx[sample], :] - x[real_min_idx[sample], :]))\n",
    "fig, axs = plt.subplots(figsize=(10,5))\n",
    "axs.hist(d)\n",
    "axs.set_xticks(range(0, x.shape[1]+1))\n",
    "axs.set_title('L1 distance between predicted model and expected model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# print(y_to_pred)\n",
    "# print(y_hats)\n",
    "# x = np.sqrt(diff_y_hats)\n",
    "# print(x)\n",
    "# print(model_e)\n",
    "# print(min_idx)\n",
    "# # compute the RMSE for each sample (there is certainly a more pythonic way to do that, but that works)\n",
    "# rmse = np.zeros((4,), dtype=np.float64)\n",
    "# for (sample_idx, model_idx) in enumerate(min_idx):\n",
    "#     print(sample_idx, model_idx)\n",
    "#     rmse += (y_hats[model_idx, sample_idx, :] - y_to_pred[sample_idx, :])**2/y_to_pred.shape[0]\n",
    "\n",
    "# rmse = np.sqrt(rmse) \n",
    "# print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# x1=(np.array([1969.4478911  , 905.53209513 ,  7.41054667 ,3656.49362621]) - np.array([2232.    ,     729.     ,      7.4000001, 3727.  ]))**2\n",
    "# x2=(np.array([ -55.492296  ,  4099.18583448   ,  7.40349391 ,   80.2708897]) - np.array([    0.    ,    4099.       ,    7.4000001  ,  0.           ]))**2\n",
    "# print(np.sqrt((x1+x2)/y_to_pred.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the weights for a model in given interval of concentrations (cell in the grid) \n",
    "cell=(0,0,0)\n",
    "l = np.where([x == models[cell]['lambda_1se'][0] for x in models[cell]['lambdau']])[0][0]\n",
    "fig, axs = plt.subplots(2, 2, sharey=False, sharex=False, tight_layout=True, figsize=(20,10))\n",
    "for ia in range(4):\n",
    "    idx = np.unravel_index(ia, axs.shape)\n",
    "    x = models[cell]['glmnet_fit']['beta'][ia][:, l]\n",
    "    axs[idx].plot(x)\n",
    "    axs[idx].set_title(\"Coeficients %s\"%names[ia])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
