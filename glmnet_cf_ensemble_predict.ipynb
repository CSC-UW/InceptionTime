{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant modules and setup for calling glmnet\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from itertools import compress\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import scipy, importlib, pprint, matplotlib.pyplot as plt, warnings\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import numpy as np\n",
    "import glmnet_python\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot\n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix = '/mnt/nfs/proj/in-vitro/Leonardo/cf_data'\n",
    "probe = 'CF025'\n",
    "probe_file = os.path.join(data_prefix, f'{probe}.npz')\n",
    "probe_data = np.load(probe_file)\n",
    "x_to_pred = probe_data['x']\n",
    "y_to_pred = probe_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '/mnt/nfs/proj/in-vitro/Leonardo/glmnet/fits/cf_alpha_1.0_2x2x2.pickle'\n",
    "with open(model_file, 'rb') as f:\n",
    "    models, ranges, analytes = pickle.load(f)\n",
    "\n",
    "names = ['DA', '5HT', 'pH', 'NE']\n",
    "\n",
    "# generate the predictions for each model\n",
    "nx = len(models)\n",
    "y_hats = np.zeros((len(models), y_to_pred.shape[0], y_to_pred.shape[1]))\n",
    "for (ix, x) in enumerate(models):\n",
    "    print(f'{ix} of {nx} ({x})... ', end='')\n",
    "    model = models[x]\n",
    "    start_time = time.time()\n",
    "    y_hat = cvglmnetPredict(model, newx = x_to_pred, s='lambda_1se') \n",
    "    y_hats[ix,:,:] = y_hat[:,:,0]\n",
    "    print(\" took %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# compute the differences to original intervals\n",
    "diff_y_hats = np.zeros(y_hats[:,:,analytes].shape)\n",
    "for (ix,x) in enumerate(models):\n",
    "    for ia in range(len(analytes)):\n",
    "        diff_y_hats[ix,:,ia] = (y_hats[ix,:,analytes[ia]] - ranges[ia][x[ia]])**2\n",
    "\n",
    "# find the model with sum of predictions closest to original intervals\n",
    "model_e = np.sqrt(np.sum(diff_y_hats,axis=2)) # take sqrt to make it easier to debug, doesn't change the ordering\n",
    "min_idx = model_e.argmin(axis=0)\n",
    "\n",
    "# compute the RMSE for each sample (there is certainly a more pythonic way to do that, but that works)\n",
    "rmse = np.zeros((4,), dtype=np.float64)\n",
    "for (sample_idx, model_idx) in enumerate(min_idx):\n",
    "    rmse += (y_hats[model_idx, sample_idx, :] - y_to_pred[sample_idx, :])**2/y_to_pred.shape[0]\n",
    "\n",
    "rmse = np.sqrt(rmse)\n",
    "\n",
    "for (armse, name) in zip(rmse,names):\n",
    "    print('%s: %4.5f'%(name,armse), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
