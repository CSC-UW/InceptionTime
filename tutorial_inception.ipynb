{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path = [p for p in sys.path if p.find('/opt/apps/software/') == -1]\n",
    "\n",
    "from utils.constants import UNIVARIATE_ARCHIVE_NAMES as ARCHIVE_NAMES\n",
    "from utils.constants import UNIVARIATE_DATASET_NAMES as DATASET_NAMES\n",
    "\n",
    "from utils.utils import read_all_datasets\n",
    "from utils.utils import transform_labels\n",
    "from utils.utils import create_directory\n",
    "from utils.utils import run_length_xps\n",
    "from utils.utils import generate_results_csv\n",
    "\n",
    "import utils\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# import keras\n",
    "# keras.backend.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/leonardo.barbosa/projects/voltametry/InceptionTime',\n",
       " '',\n",
       " '/opt/apps/software/TensorFlow/2.2.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages',\n",
       " '/opt/apps/software/h5py/2.10.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages',\n",
       " '/opt/apps/software/SciPy-bundle/2019.10-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages',\n",
       " '/opt/apps/software/Python/3.7.4-GCCcore-8.3.0/easybuild/python',\n",
       " '/opt/apps/software/h5py/2.10.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/h5py-2.10.0-py3.7-linux-x86_64.egg',\n",
       " '/home/leonardo.barbosa/.conda/envs/inception/lib/python37.zip',\n",
       " '/home/leonardo.barbosa/.conda/envs/inception/lib/python3.7',\n",
       " '/home/leonardo.barbosa/.conda/envs/inception/lib/python3.7/lib-dynload',\n",
       " '/home/leonardo.barbosa/.conda/envs/inception/lib/python3.7/site-packages',\n",
       " '/home/leonardo.barbosa/.conda/envs/inception/lib/python3.7/site-packages/glmnet_python-0.2.0-py3.7.egg',\n",
       " '/home/leonardo.barbosa/.conda/envs/inception/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/home/leonardo.barbosa/.ipython']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data():\n",
    "    x_train = datasets_dict[dataset_name][0]\n",
    "    y_train = datasets_dict[dataset_name][1]\n",
    "    x_test = datasets_dict[dataset_name][2]\n",
    "    y_test = datasets_dict[dataset_name][3]\n",
    "\n",
    "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
    "\n",
    "    # make the min to zero of labels\n",
    "    y_train, y_test = transform_labels(y_train, y_test)\n",
    "\n",
    "    # save orignal y because later we will use binary\n",
    "    y_true = y_test.astype(np.int64)\n",
    "    y_true_train = y_train.astype(np.int64)\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder()\n",
    "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "    if len(x_train.shape) == 2:  # if univariate\n",
    "        # add a dimension to make it multivariate with one dimension\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc\n",
    "\n",
    "\n",
    "def fit_classifier():\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "    classifier = create_classifier(classifier_name, input_shape, nb_classes,\n",
    "                                   output_directory) # , verbose=1\n",
    "\n",
    "    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "\n",
    "def create_classifier(classifier_name, input_shape, nb_classes, output_directory,\n",
    "                      verbose=False, build=True):\n",
    "    if classifier_name == 'nne':\n",
    "        from classifiers import nne\n",
    "        return nne.Classifier_NNE(output_directory, input_shape,\n",
    "                                  nb_classes, verbose)\n",
    "    if classifier_name == 'inception':\n",
    "        from classifiers import inception\n",
    "        return inception.Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose,\n",
    "                                              build=build)\n",
    "\n",
    "\n",
    "def get_xp_val(xp):\n",
    "    if xp == 'batch_size':\n",
    "        xp_arr = [16, 32, 128]\n",
    "    elif xp == 'use_bottleneck':\n",
    "        xp_arr = [False]\n",
    "    elif xp == 'use_residual':\n",
    "        xp_arr = [False]\n",
    "    elif xp == 'nb_filters':\n",
    "        xp_arr = [16, 64]\n",
    "    elif xp == 'depth':\n",
    "        xp_arr = [3, 9]\n",
    "    elif xp == 'kernel_size':\n",
    "        xp_arr = [8, 64]\n",
    "    else:\n",
    "        raise Exception('wrong argument')\n",
    "    return xp_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/nfs/proj/in-vitro/Leonardo/inception'\n",
    "xps = ['use_bottleneck', 'use_residual', 'nb_filters', 'depth',\n",
    "       'kernel_size', 'batch_size']\n",
    "archive_name = ARCHIVE_NAMES[0]\n",
    "datasets_dict = read_all_datasets(root_dir, archive_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\titer 0\n",
      "\t\t\tdataset_name:  Coffee\n",
      "Already_done /mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC/ Coffee\n",
      "\t\t\tdataset_name:  Meat\n",
      "Already_done /mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC/ Meat\n",
      "\t\titer 1\n",
      "\t\t\tdataset_name:  Coffee\n",
      "Already_done /mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC_itr_1/ Coffee\n",
      "\t\t\tdataset_name:  Meat\n",
      "Already_done /mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC_itr_1/ Meat\n",
      "\t\titer 2\n",
      "\t\t\tdataset_name:  Coffee\n",
      "Already_done /mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC_itr_2/ Coffee\n",
      "\t\t\tdataset_name:  Meat\n",
      "Already_done /mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC_itr_2/ Meat\n",
      "\t\titer 3\n",
      "\t\t\tdataset_name:  Coffee\n",
      "Already_done /mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC_itr_3/ Coffee\n",
      "\t\t\tdataset_name:  Meat\n",
      "Already_done /mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC_itr_3/ Meat\n",
      "\t\titer 4\n",
      "\t\t\tdataset_name:  Coffee\n",
      "Already_done /mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC_itr_4/ Coffee\n",
      "\t\t\tdataset_name:  Meat\n",
      "Already_done /mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC_itr_4/ Meat\n"
     ]
    }
   ],
   "source": [
    "# run nb_iter_ iterations of Inception on the whole TSC archive\n",
    "classifier_name = 'inception'\n",
    "nb_iter_ = 5\n",
    "\n",
    "for iter in range(nb_iter_):\n",
    "    print('\\t\\titer', iter)\n",
    "\n",
    "    trr = ''\n",
    "    if iter != 0:\n",
    "        trr = '_itr_' + str(iter)\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n",
    "\n",
    "    for dataset_name in utils.constants.dataset_names_for_archive[archive_name]:\n",
    "        print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "        x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "\n",
    "        output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "        temp_output_directory = create_directory(output_directory)\n",
    "\n",
    "        if temp_output_directory is None:\n",
    "            print('Already_done', tmp_output_directory, dataset_name)\n",
    "            continue\n",
    "\n",
    "        fit_classifier()\n",
    "\n",
    "        print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "        # the creation of this directory means\n",
    "        create_directory(output_directory + '/DONE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tdataset_name:  Coffee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/apps/software/TensorFlow/2.2.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "\t\t\t\tDONE\n",
      "\t\t\tdataset_name:  Meat\n",
      "\t\t\t\tDONE\n"
     ]
    }
   ],
   "source": [
    "# run the ensembling of these iterations of Inception\n",
    "classifier_name = 'nne'\n",
    "\n",
    "datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '/'\n",
    "\n",
    "for dataset_name in utils.constants.dataset_names_for_archive[archive_name]:\n",
    "    print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "    x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()\n",
    "\n",
    "    output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "    fit_classifier()\n",
    "\n",
    "    print('\\t\\t\\t\\tDONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.load('/mnt/nfs/proj/in-vitro/Leonardo/inception/results/inception/TSC/Meat/y_pred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meat'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(y_true==np.argmax(y_pred,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 448, 1)\n",
      "(60, 3)\n",
      "(60, 448, 1)\n",
      "(60, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
